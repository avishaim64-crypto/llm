#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
FastAPI ×©×™×¨×•×ª ×œ×ª×§×¦×•×¨ ×˜×§×¡×˜×™× ×‘×¢×‘×¨×™×ª:
1. ××§×‘×œ ×˜×§×¡×˜ ×‘×¢×‘×¨×™×ª
2. ××ª×¨×’× ×œ×× ×’×œ×™×ª ×¢× NLLB
3. ×™×•×¦×¨ ×ª×§×¦×™×¨ ×©×œ 5 × ×§×•×“×•×ª ×¢× Phi-3
4. ××—×–×™×¨ ×›×œ × ×§×•×“×” ××™×™×“×™×ª (streaming)
"""

from fastapi import FastAPI, HTTPException
from fastapi.responses import StreamingResponse
from pydantic import BaseModel
from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, TextIteratorStreamer
import ollama
import torch
import json
import re
import time
from typing import Generator, AsyncGenerator, Dict, Any
import asyncio
import threading

# ××ª×—×•×œ FastAPI
app = FastAPI(
    title="Hebrew Text Summarizer",
    description="×©×™×¨×•×ª ×œ×ª×§×¦×•×¨ ×˜×§×¡×˜×™× ×‘×¢×‘×¨×™×ª ×¢× ×ª×¨×’×•× ×•-streaming response",
    version="1.0.0"
)

# ××•×“×œ×™× ×’×œ×•×‘×œ×™×™× - × ×˜×¢×Ÿ ×¤×¢× ××—×ª ×‘×”×ª×—×œ×”
nllb_tokenizer = None
nllb_model = None


class TextRequest(BaseModel):
    """××•×“×œ ×œ×‘×§×©×ª ×ª×§×¦×•×¨ ×˜×§×¡×˜"""
    text: str  # ×”×˜×§×¡×˜ ×œ×ª×§×¦×•×¨
    max_points: int = 5  # ××¡×¤×¨ × ×§×•×“×•×ª ×”×ª×§×¦×™×¨ (1-10)
    temperature: float = 0.7  # ×¨××ª ×™×¦×™×¨×ª×™×•×ª ×”××•×“×œ (0.1-1.0): × ××•×š = ×™×•×ª×¨ ×¤×¨×“×™×§×˜×™×‘×™×œ×™, ×’×‘×•×” = ×™×•×ª×¨ ×™×¦×™×¨×ª×™
    top_p: float = 0.9  # ×¡×£ ×”×¡×ª×‘×¨×•×ª ×œ×‘×—×™×¨×ª ××™×œ×™× (0.1-1.0): × ××•×š = ××™×§×•×“ ×‘××™×œ×™× ×”×¡×‘×™×¨×•×ª ×‘×™×•×ª×¨
    max_tokens: int = 500  # ××¡×¤×¨ ××§×¡×™××œ×™ ×©×œ ×˜×•×§× ×™× (××™×œ×™× ×‘×§×™×¨×•×‘) ×œ×›×œ × ×§×•×“×”


class SummaryPoint(BaseModel):
    """××•×“×œ ×œ× ×§×•×“×ª ×ª×§×¦×™×¨ ×‘×•×“×“×ª"""
    point_number: int  # ××¡×¤×¨ ×”× ×§×•×“×” (1, 2, 3...)
    content: str  # ×ª×•×›×Ÿ ×”× ×§×•×“×” ×‘×× ×’×œ×™×ª
    timestamp: float  # ×–××Ÿ ×™×¦×™×¨×ª ×”× ×§×•×“×” (Unix timestamp)


def initialize_nllb():
    """
    ××ª×—×•×œ ××•×“×œ NLLB ×œ×ª×¨×’×•×

    ×˜×•×¢×Ÿ ××ª ×”××•×“×œ NLLB-200 (No Language Left Behind) ×©×œ Meta
    ×©×ª×•××š ×‘-200+ ×©×¤×•×ª ×›×•×œ×œ ×¢×‘×¨×™×ª â† â†’ ×× ×’×œ×™×ª
    """
    global nllb_tokenizer, nllb_model

    if nllb_tokenizer is None or nllb_model is None:
        print("ğŸ”„ ×˜×•×¢×Ÿ ××•×“×œ NLLB...")
        model_name = "facebook/nllb-200-distilled-600M"
        nllb_tokenizer = AutoTokenizer.from_pretrained(model_name)
        nllb_model = AutoModelForSeq2SeqLM.from_pretrained(model_name)
        print("âœ… ××•×“×œ NLLB × ×˜×¢×Ÿ ×‘×”×¦×œ×—×”")


def translate(text: str, tgt_lang: str, *, stream: bool = False, max_tokens: int = 500):
    """
    ×ª×¨×’×•× ×’× ×¨×™ ×‘×¢×–×¨×ª NLLB

    Args:
        text: ×”×˜×§×¡×˜ ×œ×ª×¨×’×•×
        tgt_lang: ×§×•×“ ×©×¤×ª ×”×™×¢×“ (×œ××©×œ: "eng_Latn" ×œ×× ×’×œ×™×ª, "heb_Hebr" ×œ×¢×‘×¨×™×ª)
        stream: ×”×× ×œ×”×—×–×™×¨ ×ª×•×¦××•×ª ×‘×–×¨×™××” (True) ××• ×‘×‘×ª ××—×ª (False)
        max_tokens: ××¡×¤×¨ ××§×¡×™××œ×™ ×©×œ ×˜×•×§× ×™× ×‘×ª×¨×’×•×

    Returns:
        - stream=False: ××—×–×™×¨ ××—×¨×•×–×ª ×ª×¨×’×•× ××œ××”
        - stream=True: ××—×–×™×¨ ×’× ×¨×˜×•×¨ ×©××¤×™×§ ×—×œ×§×™ ×ª×¨×’×•× ×‘×–××Ÿ ×××ª

    ×§×•×“×™ ×©×¤×•×ª ×¨×œ×•×•× ×˜×™×:
        - ×¢×‘×¨×™×ª: "heb_Hebr"
        - ×× ×’×œ×™×ª: "eng_Latn"
    """
    initialize_nllb()

    if not stream:
        # × ×ª×™×‘ ×ª×¨×’×•× ×¨×’×™×œ - ××—×–×™×¨ ×ª×•×¦××” ××œ××”
        inputs = nllb_tokenizer(
            text,
            return_tensors="pt",  # PyTorch tensors
            max_length=1024,  # ××•×¨×š ××§×¡×™××œ×™ ×©×œ ×”×§×œ×˜
            truncation=True  # ×§×˜×¢ ×˜×§×¡×˜ ××¨×•×š ××“×™
        )

        # ×”××¨×ª ×§×•×“ ×”×©×¤×” ×œID ××¡×¤×¨×™
        tgt_id = nllb_tokenizer.convert_tokens_to_ids(tgt_lang)
        if tgt_id is None:
            raise ValueError(f"×œ× × ××¦× ×§×•×“ ×©×¤×”: {tgt_lang}")

        with torch.no_grad():  # ×‘×œ×™ ×—×™×©×•×‘×™× ×œ×©×™××•×© ×¢×ª×™×“×™
            generated_tokens = nllb_model.generate(
                **inputs,
                forced_bos_token_id=tgt_id,  # ××™×œ×•×¥ ×©×¤×ª ×”×™×¢×“
                max_length=1024,  # ××•×¨×š ××§×¡×™××œ×™ ×©×œ ×”×¤×œ×˜
                num_beams=3,  # ×›××•×ª × ×ª×™×‘×™× ×œ×—×§×™×¨×”
                early_stopping=True  # ×¢×¦×•×¨ ×›×©××’×™×¢×™× ×œ×¡×•×£ ×”×’×™×•× ×™
            )

        translation = nllb_tokenizer.batch_decode(generated_tokens, skip_special_tokens=True)[0]
        return translation.strip()

    # × ×ª×™×‘ ×¡×˜×¨×™××™× ×’ - ××—×–×™×¨ ×’× ×¨×˜×•×¨ ×œ×ª×¨×’×•× ×‘×–××Ÿ ×××ª
    def _stream_generator():
        """×’× ×¨×˜×•×¨ ×¤× ×™××™ ×œ×¡×˜×¨×™××™× ×’ ×ª×¨×’×•×"""
        inputs = nllb_tokenizer(
            text,
            return_tensors="pt",
            max_length=max_tokens,
            truncation=True
        )

        model_device = next(nllb_model.parameters()).device
        input_ids = inputs["input_ids"][:1].to(model_device)#×”××¡×¤×¨×™× ×©××™×™×¦×’×™× ××ª ×”×ª×•×›×Ÿ
        attention_mask = inputs["attention_mask"][:1].to(model_device) #×ª×•×›×Ÿ ×××™×ª×™

        # ×‘×“×™×§×ª ×ª×§×™× ×•×ª ×§×•×“ ×”×©×¤×”
        tgt_id = nllb_tokenizer.convert_tokens_to_ids(tgt_lang)
        if tgt_id is None:
            raise ValueError(f"×œ× × ××¦× ×§×•×“ ×©×¤×”: {tgt_lang}")

        # ×”×’×“×¨×ª TextIteratorStreamer ×œ×§×‘×œ×ª ×˜×•×§× ×™× ×‘×–××Ÿ ×××ª
        streamer = TextIteratorStreamer(
            nllb_tokenizer,
            skip_special_tokens=True,  # ×œ× ×œ×”×¦×™×’ ×˜×•×§× ×™× ×˜×›× ×™×™×
            skip_prompt=True  # ×œ× ×œ×”×¦×™×’ ××ª ×”×§×œ×˜ ×”××§×•×¨×™
        )

        generation_kwargs = dict(
            input_ids=input_ids,
            attention_mask=attention_mask,
            forced_bos_token_id=tgt_id,
            max_new_tokens=max_tokens,  # ××¡×¤×¨ ×˜×•×§× ×™× ×—×“×©×™× ××§×¡×™××œ×™
            do_sample=False,  # ×“×˜×¨××™× ×™×¡×˜×™ (×œ× ×¨× ×“×•××œ×™)
            num_beams=1,  # ×œ×œ× beam search ×œ×¡×˜×¨×™××™× ×’ ××”×™×¨
            streamer=streamer
        )

        def _run_generate():
            """×¤×•× ×§×¦×™×” ×œ×”×¨×¦×ª ×”×’× ×¨×¦×™×” ×‘×ª×”×œ×™×›×•×Ÿ × ×¤×¨×“"""
            with torch.no_grad():
                nllb_model.generate(**generation_kwargs)

        # ×”×¨×¦×” ×‘×ª×”×œ×™×›×•×Ÿ × ×¤×¨×“ ×›×“×™ ×œ× ×œ×—×¡×•× ××ª ×”×¡×˜×¨×™××™× ×’
        thread = threading.Thread(target=_run_generate)
        thread.start()

        # ×–×¨×™××ª ×”×˜×•×§× ×™× ×”×—×“×©×™× ×‘×¨×’×¢ ×©×”× × ×•×¦×¨×™×
        for new_text in streamer:
            yield new_text

    return _stream_generator()


def check_ollama_connection() -> bool:
    """
    ×‘×“×™×§×ª ×—×™×‘×•×¨ ×œ-Ollama

    Returns:
        True ×× Ollama ×–××™×Ÿ, False ××—×¨×ª
    """
    try:
        ollama.list()  # ×‘×“×™×§×” ×¤×©×•×˜×” ×©×œ ×–××™× ×•×ª ×”×©×™×¨×•×ª
        return True
    except Exception:
        return False


def ensure_phi3_model() -> bool:
    """
    ×•×™×“×•× ×©××•×“×œ Phi-3 ×–××™×Ÿ ×‘-Ollama

    Returns:
        True ×× ×”××•×“×œ ×–××™×Ÿ ××• ×”×•×¨×“ ×‘×”×¦×œ×—×”, False ××—×¨×ª

    Note:
        Phi-3-mini ×”×•× ××•×“×œ ×©×¤×” ×§×˜×Ÿ ×•×™×¢×™×œ ×©×œ Microsoft
        ××ª××™× ×œ×ª×§×¦×•×¨×™× ×•××©×™××•×ª NLP ×‘×¡×™×¡×™×•×ª
    """
    try:
        models = ollama.list()
        available_models = [m.get("model", "") for m in models.get("models", [])]

        model_name = "phi3:mini"
        if not any(model_name in model for model in available_models):
            print(f"ğŸ”„ ××•×¨×™×“ ××•×“×œ {model_name}...")
            ollama.pull(model_name)
            print(f"âœ… ××•×“×œ {model_name} ×”×•×¨×“ ×‘×”×¦×œ×—×”")

        return True
    except Exception as e:
        print(f"âŒ ×©×’×™××” ×‘×”×•×¨×“×ª ××•×“×œ: {e}")
        return False


async def stream_summary_points_with_phi3(english_text: str, num_points: int = 5, *, temperature: float = 0.7,
                                          top_p: float = 0.9, max_tokens: int = 500):
    """
    ×–×¨×™××ª × ×§×•×“×•×ª ×ª×§×¦×™×¨ ×™×©×™×¨×•×ª ×-Ollama (Phi-3) ×‘×–××Ÿ ×××ª

    Args:
        english_text: ×”×˜×§×¡×˜ ×‘×× ×’×œ×™×ª ×œ×ª×§×¦×•×¨
        num_points: ××¡×¤×¨ × ×§×•×“×•×ª ×”×ª×§×¦×™×¨ ×”×¨×¦×•×™
        temperature: ×¨××ª ×™×¦×™×¨×ª×™×•×ª (0.1-1.0)
            - 0.1-0.3: ×××•×“ ×©××¨× ×™ ×•×¤×¨×“×™×§×˜×™×‘×™×œ×™
            - 0.4-0.7: ×××•×–×Ÿ (××•××œ×¥)
            - 0.8-2.0: ×™×¦×™×¨×ª×™ ×•×‘×œ×ª×™ ×¦×¤×•×™
        top_p: ×”×¡×ª×‘×¨×•×ª ××¦×˜×‘×¨×ª ×œ×‘×—×™×¨×ª ××™×œ×™× (0.1-1.0)
            - 0.1-0.5: ××™×§×•×“ ×‘××™×œ×™× ×”×¡×‘×™×¨×•×ª ×‘×™×•×ª×¨
            - 0.6-0.9: ×××•×–×Ÿ (××•××œ×¥)
            - 0.95-1.0: ×©×™×§×•×œ ×©×œ ×›×œ ×”××™×œ×™× ×”××¤×©×¨×™×•×ª
        max_tokens: ××¡×¤×¨ ××§×¡×™××œ×™ ×©×œ ×˜×•×§× ×™× ×œ×›×œ × ×§×•×“×”

    Yields:
        tuples ×©×œ (××¡×¤×¨_× ×§×•×“×”, ×ª×•×›×Ÿ_×”× ×§×•×“×”) ×¢×‘×•×¨ ×›×œ × ×§×•×“×” ×©×”×•×©×œ××”

    Note:
        ×”×¤×•× ×§×¦×™×” ××–×”×” ×”×ª×—×œ×” ×•×¡×™×•× ×©×œ ×›×œ × ×§×•×“×” ×××•×¡×¤×¨×ª ×‘×–××Ÿ ×××ª
        ×•××©×™×‘×” ×›×œ × ×§×•×“×” ××™×“ ×›×©×”×™× ××•×›× ×”, ×œ×œ× ×”××ª× ×” ×œ×¡×™×•× ×›×œ ×”×˜×§×¡×˜
    """
    # ×‘× ×™×™×ª prompt ××•×‘× ×” ×œ××•×“×œ
    prompt = f"""Please create a summary of the following text in exactly {num_points} numbered bullet points.
Each point must be a single line, start with an explicit number like "1. ", "2. ", etc., and be at most {max_tokens} characters long.
Do not add any prose before or after the list. Only output the numbered list.

Text to summarize:
{english_text}

Summary:"""

    # ××©×ª× ×™× ×œ×–×™×”×•×™ ×’×‘×•×œ×•×ª × ×§×•×“×•×ª
    expected_index = 1  # ×”× ×§×•×“×” ×”×‘××” ×©×× ×• ××¦×¤×™× ×œ×”
    buffer = ""  # ××’×™×¨×ª ×˜×§×¡×˜ ×¢×“ ×©× ×§×•×“×” ××•×©×œ××ª

    try:
        # ×™×¦×™×¨×ª queue ××¡×™× ×›×¨×•× ×™ ×œ×”×¢×‘×¨×ª × ×ª×•× ×™× ×‘×™×Ÿ ×ª×”×œ×™×›×•× ×™×
        queue: asyncio.Queue = asyncio.Queue(maxsize=0)

        def producer():
            """
            ×¤×•× ×§×¦×™×ª producer ×œ×§×‘×œ×ª streaming response ×-Ollama
            ×¨×¦×” ×‘×ª×”×œ×™×›×•×Ÿ × ×¤×¨×“ ×›×“×™ ×œ× ×œ×—×¡×•× ××ª ×”××™×¨×•×¢ ×”×¨××©×™
            """
            try:
                stream = ollama.chat(
                    model="phi3:mini",
                    messages=[{'role': 'user', 'content': prompt}],
                    options={
                        'temperature': float(temperature),  # ×”××¨×” ××¤×•×¨×©×ª ×œ×‘×˜×™×—×•×ª
                        'top_p': float(top_p),
                        'max_tokens': int(max_tokens)
                    },
                    stream=True  # ×”×¤×¢×œ×ª streaming mode
                )

                # ×”×¢×‘×¨×ª ×›×œ chunk ×œqueue
                for chunk in stream:
                    try:
                        queue.put_nowait(chunk)
                    except Exception:
                        break

            except Exception as e:
                # ×©×œ×™×—×ª ×©×’×™××” ×œqueue
                try:
                    queue.put_nowait({'error': str(e)})
                except Exception:
                    pass
            finally:
                # ×¡×™×’× ×œ ×¡×™×•×
                try:
                    queue.put_nowait(None)
                except Exception:
                    pass

        # ×”×¤×¢×œ×ª producer ×‘×ª×”×œ×™×›×•×Ÿ × ×¤×¨×“
        threading.Thread(target=producer, daemon=True).start()

        # ×œ×•×œ××” ×¨××©×™×ª ×œ×¢×™×‘×•×“ chunks
        while True:
            item = await queue.get()

            # ×‘×“×™×§×ª ×¡×™×•× ××• ×©×’×™××”
            if item is None:
                break
            if isinstance(item, dict) and 'error' in item:
                raise HTTPException(detail=item['error'])

            # ×—×™×œ×•×¥ ×”×˜×§×¡×˜ ××”-chunk
            piece = item.get('message', {}).get('content', '')
            if not piece:
                continue

            buffer += piece

            # × ×™×¡×™×•×Ÿ ×œ×–×”×•×ª × ×§×•×“×•×ª ×©×”×•×©×œ××•
            # ××œ×’×•×¨×™×ª×: ×—×¤×© "N. " ×•××—×¨×™×• "N+1. " - ××” ×©×‘×™× ×™×”× ×–×• × ×§×•×“×” ×©×œ××”
            while True:
                start_marker = f"{expected_index}. "
                next_marker = f"{expected_index + 1}. "

                start_pos = buffer.find(start_marker)
                if start_pos == -1:
                    break  # ×œ× ××¦×× ×• ××ª ×”×”×ª×—×œ×” ×©×œ ×”× ×§×•×“×” ×”× ×•×›×—×™×ª

                next_pos = buffer.find(next_marker, start_pos + len(start_marker))
                if next_pos == -1:
                    break  # ×œ× ××¦×× ×• ××ª ×”×”×ª×—×œ×” ×©×œ ×”× ×§×•×“×” ×”×‘××”

                # ×—×™×œ×•×¥ ×ª×•×›×Ÿ ×”× ×§×•×“×” (××” ×©×‘×™×Ÿ ×”×”×ª×—×œ×•×ª)
                point_text = buffer[start_pos + len(start_marker):next_pos]
                point_text = point_text.strip().strip("-â€¢* ")  # × ×™×§×•×™ ×ª×•×•×™× ××™×•×ª×¨×™×

                if point_text:
                    yield expected_index, point_text
                    expected_index += 1

                # ×”×¡×¨×ª ×”×—×œ×§ ×©×¢×•×‘×“ ××”buffer
                buffer = buffer[next_pos:]

                if expected_index > num_points:
                    break

            if expected_index > num_points:
                break

        # ×˜×™×¤×•×œ ×‘× ×§×•×“×” ××—×¨×•× ×” ×©×¢×œ×•×œ×” ×œ×”×™×©××¨ ×‘buffer
        if expected_index <= num_points:
            start_marker = f"{expected_index}. "
            start_pos = buffer.find(start_marker)
            if start_pos != -1:
                tail = buffer[start_pos + len(start_marker):]
                tail = tail.strip().strip("-â€¢* ")
                if tail:
                    yield expected_index, tail

    except Exception as e:
        raise HTTPException(e)


async def generate_streaming_summary(request: TextRequest) -> AsyncGenerator[str, None]:
    """
    ×™×¦×™×¨×ª ×ª×§×¦×™×¨ ×¢× streaming response

    ×”×¤×•× ×§×¦×™×” ××‘×¦×¢×ª ××ª ×”×ª×”×œ×™×š ×”×‘×:
    1. ×‘×“×™×§×ª ×—×™×‘×•×¨ ×œ-Ollama
    2. ×•×™×“×•× ×–××™× ×•×ª ××•×“×œ Phi-3
    3. ×ª×¨×’×•× ×”×˜×§×¡×˜ ×”×¢×‘×¨×™ ×œ×× ×’×œ×™×ª
    4. ×™×¦×™×¨×ª ×ª×§×¦×™×¨ × ×§×•×“×•×ª ×‘×× ×’×œ×™×ª (×‘×¡×˜×¨×™××™× ×’)
    5. ×ª×¨×’×•× ×›×œ × ×§×•×“×” ×—×–×¨×” ×œ×¢×‘×¨×™×ª (×‘×¡×˜×¨×™××™× ×’)

    Args:
        request: ××•×‘×™×™×§×˜ TextRequest ×¢× ×¤×¨××˜×¨×™ ×”×ª×§×¦×™×¨

    Yields:
        ××—×¨×•×–×•×ª JSON ×‘×¤×•×¨××˜ Server-Sent Events
        ×›×œ ××—×¨×•×–×ª ××ª×—×™×œ×” ×‘-"data: " ×•××¡×ª×™×™××ª ×‘-"\n\n"

    JSON Types ×©××•×©×‘×™×:
        - {"error": "×”×•×“×¢×ª ×©×’×™××”"}
        - {"english_text": "×”×ª×¨×’×•× ×”××œ× ×œ×× ×’×œ×™×ª"}
        - {"status": "×”×•×“×¢×ª ×¡×˜×˜×•×¡"}
        - {"summary_point": {point_number, content, timestamp}}
        - {"summary_point_hebrew_piece": {point_number, piece}}
        - {"summary_point_hebrew": {point_number, content}}
        - {"status": "completed", "total_points": ××¡×¤×¨}
    """
    try:
        # ×©×œ×‘ 1: ×‘×“×™×§×ª ×—×™×‘×•×¨ ×œ-Ollama
        if not check_ollama_connection():
            error_msg = {"error": "×œ× × ×™×ª×Ÿ ×œ×”×ª×—×‘×¨ ×œ-Ollama. ×•×“× ×©×”×©×™×¨×•×ª ×¨×¥: ollama serve"}
            yield f"data: {json.dumps(error_msg, ensure_ascii=False)}\n\n"
            return

        # ×©×œ×‘ 2: ×•×™×“×•× ××•×“×œ Phi-3
        if not ensure_phi3_model():
            error_msg = {"error": "×œ× × ×™×ª×Ÿ ×œ×˜×¢×•×Ÿ ××ª ××•×“×œ Phi-3"}
            yield f"data: {json.dumps(error_msg, ensure_ascii=False)}\n\n"
            return

        # ×©×œ×‘ 3: ×ª×¨×’×•× ×œ×× ×’×œ×™×ª (×œ×œ× ×¡×˜×¨×™××™× ×’ - ×œ×§×‘×œ×ª ×ª×•×¦××” ××œ××”)
        try:
            english_text = translate(request.text, "eng_Latn", stream=False)
        except Exception as e:
            error_msg = {"error": f"×©×’×™××” ×‘×ª×¨×’×•×: {str(e)}"}
            yield f"data: {json.dumps(error_msg, ensure_ascii=False)}\n\n"
            return

        # ×©×™×“×•×¨ ×”×ª×¨×’×•× ×”××œ× ×œ×œ×§×•×—
        yield f"data: {json.dumps({'english_text': english_text}, ensure_ascii=False)}\n\n"

        # ×©×œ×‘ 4: ×™×¦×™×¨×ª ×ª×§×¦×™×¨ ×‘×¡×˜×¨×™××™× ×’
        yield f"data: {json.dumps({'status': '×™×•×¦×¨ ×ª×§×¦×™×¨ ×‘×–××Ÿ ×××ª...'}, ensure_ascii=False)}\n\n"

        sent_points = 0
        try:
            # ×–×¨×™××ª × ×§×•×“×•×ª ×ª×§×¦×™×¨ ×-Phi-3
            async for idx, point in stream_summary_points_with_phi3(
                    english_text,
                    request.max_points,
                    temperature=request.temperature,
                    top_p=request.top_p,
                    max_tokens=request.max_tokens
            ):
                # ×™×¦×™×¨×ª ××•×‘×™×™×§×˜ × ×§×•×“×”
                summary_point = SummaryPoint(
                    point_number=idx,
                    content=point,
                    timestamp=time.time()
                )
                sent_points += 1

                # ×©×œ×™×—×ª ×”× ×§×•×“×” ×‘×× ×’×œ×™×ª
                yield f"data: {json.dumps({'summary_point': summary_point.dict()}, ensure_ascii=False)}\n\n"

                # ×©×œ×‘ 5: ×ª×¨×’×•× ×”× ×§×•×“×” ×œ×¢×‘×¨×™×ª ×‘×¡×˜×¨×™××™× ×’
                heb_text = ""
                try:
                    # ×ª×¨×’×•× ×—×œ×§-×—×œ×§ ×¢× streaming
                    for piece in translate(point, "heb_Hebr", stream=True, max_tokens=request.max_tokens):
                        heb_text += piece
                        # ×©×œ×™×—×ª ×›×œ ×—×œ×§ ×‘× ×¤×¨×“
                        yield f"data: {json.dumps({'summary_point_hebrew_piece': {'point_number': idx, 'piece': piece}}, ensure_ascii=False)}\n\n"

                    # ×©×œ×™×—×ª ×”×ª×¨×’×•× ×”××œ× ×œ×¢×‘×¨×™×ª
                    yield f"data: {json.dumps({'summary_point_hebrew': {'point_number': idx, 'content': heb_text}}, ensure_ascii=False)}\n\n"

                except Exception as te:
                    # ×©×’×™××” ×‘×ª×¨×’×•× × ×§×•×“×” ×¡×¤×¦×™×¤×™×ª
                    yield f"data: {json.dumps({'error': f'×©×’×™××” ×‘×ª×¨×’×•× × ×§×•×“×” {idx} ×œ×¢×‘×¨×™×ª: {str(te)}'}, ensure_ascii=False)}\n\n"

                if sent_points >= request.max_points:
                    break

            # ×”×•×“×¢×ª ×¡×™×•×
            yield f"data: {json.dumps({'status': 'completed', 'total_points': sent_points}, ensure_ascii=False)}\n\n"

        except Exception as e:
            error_msg = {"error": f"×©×’×™××” ×‘×™×¦×™×¨×ª ×ª×§×¦×™×¨ ×‘×¡×˜×¨×™××™× ×’: {str(e)}"}
            yield f"data: {json.dumps(error_msg, ensure_ascii=False)}\n\n"
            return

    except Exception as e:
        error_msg = {"error": f"×©×’×™××” ×›×œ×œ×™×ª: {str(e)}"}
        yield f"data: {json.dumps(error_msg, ensure_ascii=False)}\n\n"


@app.on_event("startup")
async def startup_event():
    """
    ××ª×—×•×œ ×”×©×™×¨×•×ª

    ××•×¤×¢×œ ××•×˜×•××˜×™×ª ×›×©×”×©×™×¨×•×ª ×¢×•×œ×”
    ×˜×•×¢×Ÿ ××ª ××•×“×œ×™ ×”×ª×¨×’×•× ××¨××© ×œ×‘×™×¦×•×¢×™× ×˜×•×‘×™× ×™×•×ª×¨
    """
    print("ğŸš€ ××ª×—×™×œ ×©×™×¨×•×ª ×ª×§×¦×•×¨ ×˜×§×¡×˜×™×")
    print("ğŸ”„ ×˜×•×¢×Ÿ ××•×“×œ×™ ×ª×¨×’×•×...")
    initialize_nllb()


@app.get("/health")
async def health_check():
    """
    ×‘×“×™×§×ª ×ª×§×™× ×•×ª ×”×©×™×¨×•×ª

    Returns:
        ××™×“×¢ ×¢×œ ×¡×˜×˜×•×¡ ×”×©×™×¨×•×ª ×•×¨×›×™×‘×™×• ×”×©×•× ×™×

    Response:
        {
            "status": "healthy/unhealthy",
            "services": {
                "nllb_translation": bool,
                "ollama_phi3": bool
            },
            "timestamp": float
        }
    """
    ollama_status = check_ollama_connection()

    return {
        "status": "healthy" if ollama_status else "unhealthy",
        "services": {
            "nllb_translation": nllb_model is not None,
            "ollama_phi3": ollama_status
        },
        "timestamp": time.time()
    }


@app.post("/summarize")
async def summarize_text(request: TextRequest):
    """
    ×ª×§×¦×•×¨ ×˜×§×¡×˜ ×‘×¢×‘×¨×™×ª ×¢× streaming response

    ××§×‘×œ ×˜×§×¡×˜ ×‘×¢×‘×¨×™×ª ×•××—×–×™×¨ ×ª×§×¦×™×¨ ×©×œ ×¢×“ 10 × ×§×•×“×•×ª.
    ×›×œ × ×§×•×“×” × ×©×œ×—×ª ××™×“ ×›×©×”×™× ××•×›× ×”, ×’× ×‘×× ×’×œ×™×ª ×•×’× ×‘×ª×¨×’×•× ×œ×¢×‘×¨×™×ª.

    Args:
        request: TextRequest ×¢× ×”×¤×¨××˜×¨×™× ×”×‘××™×:
            - text: ×”×˜×§×¡×˜ ×œ×ª×§×¦×•×¨ (××™× ×™××•× 50 ×ª×•×•×™×)
            - max_points: ××¡×¤×¨ × ×§×•×“×•×ª (1-10, ×‘×¨×™×¨×ª ××—×“×œ: 5)
            - temperature: ×¨××ª ×™×¦×™×¨×ª×™×•×ª (0.1-1.0, ×‘×¨×™×¨×ª ××—×“×œ: 0.7)
            - top_p: ×¡×£ ×”×¡×ª×‘×¨×•×ª (0.1-1.0, ×‘×¨×™×¨×ª ××—×“×œ: 0.9)
            - max_tokens: ××§×¡ ×˜×•×§× ×™× ×œ× ×§×•×“×” (×‘×¨×™×¨×ª ××—×“×œ: 500)

    Returns:
        StreamingResponse: ×–×¨× ×©×œ ××™×¨×•×¢×™× ×‘-JSON format

    Raises:
        HTTPException 400: ×× ×”×˜×§×¡×˜ ×¨×™×§, ×§×¦×¨ ××“×™, ××• ×¤×¨××˜×¨×™× ×œ× ×ª×§×™× ×™×

    Example:
        POST /summarize
        {
            "text": "×˜×§×¡×˜ ××¨×•×š ×‘×¢×‘×¨×™×ª...",
            "max_points": 5,
            "temperature": 0.5
        }

    Stream Events:
        - english_text: ×”×ª×¨×’×•× ×”××œ× ×œ×× ×’×œ×™×ª
        - status: ×”×•×“×¢×•×ª ×¡×˜×˜×•×¡
        - summary_point: × ×§×•×“×” ×‘×× ×’×œ×™×ª
        - summary_point_hebrew_piece: ×—×œ×§ ××ª×¨×’×•× ×œ×¢×‘×¨×™×ª
        - summary_point_hebrew: ×ª×¨×’×•× ×©×œ× ×œ×¢×‘×¨×™×ª
        - error: ×”×•×“×¢×•×ª ×©×’×™××”
        - completed: ×¡×™×•× ×”×¢×™×‘×•×“ ×¢× ×¡×™×›×•×
    """

    # ×‘×“×™×§×•×ª ×ª×§×™× ×•×ª ×”×§×œ×˜
    if not request.text.strip():
        raise HTTPException(status_code=400, detail="×˜×§×¡×˜ ×œ× ×™×›×•×œ ×œ×”×™×•×ª ×¨×™×§")

    if len(request.text) < 50:
        raise HTTPException(status_code=400, detail="×˜×§×¡×˜ ×§×¦×¨ ××“×™ ×œ×ª×§×¦×•×¨ (××™× ×™××•× 50 ×ª×•×•×™×)")

    if request.max_points < 1 or request.max_points > 10:
        raise HTTPException(status_code=400, detail="××¡×¤×¨ × ×§×•×“×•×ª ×”×ª×§×¦×™×¨ ×—×™×™×‘ ×œ×”×™×•×ª ×‘×™×Ÿ 1-10")

    if not (0.1 <= request.temperature <= 2.0):
        raise HTTPException(status_code=400, detail="temperature ×—×™×™×‘ ×œ×”×™×•×ª ×‘×™×Ÿ 0.1-2.0")

    if not (0.1 <= request.top_p <= 1.0):
        raise HTTPException(status_code=400, detail="top_p ×—×™×™×‘ ×œ×”×™×•×ª ×‘×™×Ÿ 0.1-1.0")

    if request.max_tokens < 50 or request.max_tokens > 2000:
        raise HTTPException(status_code=400, detail="max_tokens ×—×™×™×‘ ×œ×”×™×•×ª ×‘×™×Ÿ 50-2000")

    return StreamingResponse(
        generate_streaming_summary(request),
        media_type="text/event-stream",
        headers={
            "Cache-Control": "no-cache",  # ×× ×™×¢×ª cache ×‘×“×¤×“×¤×Ÿ
            "Connection": "keep-alive",  # ×©××™×¨×ª ×”×—×™×‘×•×¨ ×¤×ª×•×—
            "X-Accel-Buffering": "no"  # ×× ×™×¢×ª buffering ×‘-nginx
        }
    )


# × ×§×•×“×ª ×›× ×™×¡×” ×œ×¨×™×¦×” ××§×•××™×ª
if __name__ == "__main__":
    import uvicorn

    print("ğŸš€ ××¤×¢×™×œ ×©×™×¨×•×ª FastAPI ×¢×œ ×¤×•×¨×˜ 8000")
    print("ğŸ“– ×ª×™×¢×•×“ API ×–××™×Ÿ ×‘: http://localhost:8000/docs")
    print("ğŸ¥ ×‘×“×™×§×ª ×ª×§×™× ×•×ª: http://localhost:8000/health")
    print("ğŸ¯ ×©×™××•×©:")
    print("   POST /summarize ×¢× JSON: {\"text\": \"×˜×§×¡×˜ ×‘×¢×‘×¨×™×ª...\"}")
    print("   Response: Server-Sent Events stream")

    uvicorn.run(app, host="0.0.0.0", port=8000)